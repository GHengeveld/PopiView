PopiView
========

In this document we will describe the PopiView API from a birdseye perspective.
The goal of PopiView is to monitor the current popularity of a webpage
in respect to the historical popularity.

Configuration
-------------
Config settings can be changed through settings.ini. For this doctest we use
the following config dictionary, which would normally be generated by WebOb.

>>> config = {'sparams': {'bing': 'q', 'google': 'q', 'yahoo': 'p'}, 'dbhost':
... 'localhost', 'dbpass': 'qqrs', 'urlmap': {'index': 'index',
... 'keywordcloud.json': 'keywordcloud', 'cleardata': 'cleardata', 'component':
... 'get_component', 'hitmonitor.json': 'hitmonitor', 'randomdata':
... 'randomdata', 'deviators.json': 'deviators', 'image.gif': 'log_hit',
... 'dummydata': 'dummydata'}, 'dbuser': 'root', 'dbname': 'popiview', 
... 'recenthits_size': '200', 'title_strip': '| brusselnieuws.be'}

Hits
----

When a user visits a page, we retrieve the following data:

* url of page
* referrer
* timestamp

Using this data we can construct a Hit object,

>>> from popiview.hit import Hit
>>> hit = Hit(config, u'http://mysite.com/page')
>>> hit.url()
u'http://mysite.com/page'

A hit object normalizes the url, so it can strip of the slash at
the end of the url.

>>> hit = Hit(config, u'http://mysite.com/page/')
>>> hit.url()
u'http://mysite.com/page'

The referrer string can also be passed to the hit class, which will
be analyzed to see if there are any keywords.

>>> hit = Hit(config, u'http://www.mysite.com/page',
...           referrer='http://www.google.com?q=cool%20page')
>>> hit.keywords()
[u'cool', u'page']

It is also possible to specify a datetime when the request was made.
This is useful in unittesting, and to add historic data from external
sources like google-gdata

>>> hit = Hit(config, u'http://www.mysite.com/page', timestamp=1285689362)

The timestamp should be a unix timestamp of UTC time.


Storing Hits
------------

Hits need to be stored, so we can sum them and analyse them over time.
We first create a storage object

>>> from popiview.storage import MemoryStorage
>>> storage = MemoryStorage(config)

Adding a hit is quite simple:

>>> storage.add_hit(Hit(config, u'http://www.mysite.com/page'))

We can retrieve a list of all pages accessed

>>> storage.list_urls()
[u'http://www.mysite.com/page']

Lets add another hit from a long time ago

>>> storage.add_hit(Hit(config, u'http://www.mysite.com/page2',
...                     timestamp=1000))

Now we should have two urls stored

>>> storage.list_urls()
[u'http://www.mysite.com/page', u'http://www.mysite.com/page2']

We can also ask the storage for a list of urls that have been
accessed within a certain period. This should return only our first entry,
because the other one had an older timestamp.

>>> storage.list_urls(start_time=9000)
[u'http://www.mysite.com/page']

Lets empty the hit log now and create some dummy hits with different timestamps

>>> storage.clear_hits()
>>> storage.add_hit(Hit(config, u'http://www.mysite.com/page', timestamp=1285050010,
...                     referrer='http://google.com?q=cool%20page'))
>>> storage.add_hit(Hit(config, u'http://www.mysite.com/page', timestamp=1285060010,
...                     referrer='http://google.com?q=cool'))
>>> storage.add_hit(Hit(config, u'http://www.mysite.com/page', timestamp=1285070010,
...                     referrer='http://google.com?q=page'))

We can now get the number of hits on that page for a specific period

>>> storage.get_hitcount(u'http://www.mysite.com/page',
...                  start_time=1285050000,
...                  end_time=1285070000)
2

We can also get an overview of all urls and their hitcounts

>>> storage.get_hitcounts(start_time=1285050000, end_time=1285070000,
...                       qfield='hit_url')
{u'http://www.mysite.com/page': 2}

It's also possible to get the keywords for a specific page within a
certain period

>>> storage.get_keywords(u'http://www.mysite.com/page',
...                      start_time=1285050000,
...                      end_time=1285070000)
{u'page': 1, u'cool': 2}


Determining Popularity
----------------------

Let's analyze the storage with all its hits.
To create an Analyzer instance, we need to pass
a storage instance.

An analyzer looks at the hits for a page within
a certain period, and compares those numbers with
the history of that page.

For example, if we want to analyze all hits for a
single day, and compare them with hits from the last
month, we can set `short_term_max` to yesterday and
`long_term_max` to 1 month.

What the analyzer does is:

- Ask the storage for a list of urls that were accessed in the
  short term period, optionally giving a treshold
- For each of these urls, get the number of hist from both the
  short term and the long term period
- Hand of these numbers to (one of) the popularity_algorithm(s)

>>> from popiview.analyzer import Analyzer
>>> analyzer = Analyzer(storage)

Now that we have the analyzer, we can ask it what the trends are
on the website. First we will need to create some dummy data.

>>> from popiview.dummy import Dummy
>>> dummy = Dummy(config, storage)
>>> dummy.create_hits_linear(u'http://www.mysite.com/page',
...                          start_time=0, end_time=10000,
...                          start_hits_per_hour=0, end_hits_per_hour=50,
...                          referrer='http://google.com?q=cool%20page')
>>> storage.list_urls(unique=True)
[u'http://www.mysite.com/page']

Add some more dummy data

>>> dummy.create_hits_linear(u'http://www.mysite.com/page',
...                          start_time=5000, end_time=10000,
...                          start_hits_per_hour=0, end_hits_per_hour=50,
...                          referrer='http://google.com?q=cool')
>>> dummy.create_hits_linear(u'http://www.mysite.com/page2',
...                          start_time=0, end_time=10000)
>>> dummy.create_hits_linear(u'http://www.mysite.com/page3',
...                          start_time=0, end_time=10000,
...                          start_hits_per_hour=0, end_hits_per_hour=75)
>>> dummy.create_hits_linear(u'http://www.mysite.com/page4',
...                          start_time=0, end_time=10000,
...                          start_hits_per_hour=200, end_hits_per_hour=0)

Now we can test the analyzer

>>> analyzer.get_top_deviators(qfield='hit_url', 
...     start_time=0, boundary_time=5000, end_time=10000)
[{'name': u'http://www.mysite.com/page', 'value': 412},
 {'name': u'http://www.mysite.com/page3', 'value': 200},
 {'name': u'http://www.mysite.com/page4', 'value': -66},
 {'name': u'http://www.mysite.com/page2', 'value': 1}]

By default they are sorted by absolute value, meaning -66 comes between
203 and 0. We can disable this behavior:

>>> analyzer.get_top_deviators(sort_absolute=False, qfield='hit_url', 
...     start_time=0, boundary_time=5000, end_time=10000)
[{'name': u'http://www.mysite.com/page', 'value': 412},
 {'name': u'http://www.mysite.com/page3', 'value': 200},
 {'name': u'http://www.mysite.com/page2', 'value': 1},
 {'name': u'http://www.mysite.com/page4', 'value': -66}]

If we want just the top # of deviators we can set a limit:

>>> analyzer.get_top_deviators(limit=1, qfield='hit_url', 
...     start_time=0, boundary_time=5000, end_time=10000)
[{'name': u'http://www.mysite.com/page', 'value': 412}]


Keyword Cloud
-------------

A separate part of the application is the keyword cloud. Because we want to set
a different timespan for this we create a new Analyzer first.

>>> keywordanalyzer = Analyzer(storage)

We can now get a keyword 'cloud', which is a list of keywords and their
relative sizes and and the searchphrases in which they occurred.

>>> keywordanalyzer.get_keyword_cloud()
[(u'cool', 60.0, [u'cool', u'cool page']),
 (u'page', 40.0, [u'cool page', u'page'])]

We can specify a minimum hitcount for the keyword, so only keywords with at
least so many hits are returned:

>>> keywordanalyzer.get_keyword_cloud(minimum_count=80)
[(u'cool', 100.0, [u'cool', u'cool page'])]

Or with a limit:

>>> keywordanalyzer.get_keyword_cloud(limit=1)
[(u'cool', 100.0, [u'cool', u'cool page'])]

We can adjust the percentage by setting minimum and/or maximum values:

>>> keywordanalyzer.get_keyword_cloud(minimum_pct=25, maximum_pct=175)
[(u'cool', 115.0, [u'cool', u'cool page']),
 (u'page', 85.0, [u'cool page', u'page'])]
